# í—¬ì±„ë„ ìë™ ì¶”ì²œ ì›¹ ëŒ€ì‹œë³´ë“œ (Streamlit)
import pandas as pd
import streamlit as st
import requests
import json
import os
from bs4 import BeautifulSoup

# DFGEAR ì‹¤ì‹œê°„ ë“í…œ ì •ë³´ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_dfgear():
    url = "https://dfgear.kr/bbs/board.php?bo_table=drop"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        rows = soup.select(".tbl_head01 tbody tr")

        data = []
        for row in rows:
            cols = row.select("td")
            if len(cols) >= 3:
                channel = cols[2].text.strip()
                gear = cols[1].text.strip()
                data.append({"ì±„ë„": channel, "ì¥ë¹„": gear, "ì¹´í…Œê³ ë¦¬": categorize_channel(channel)})

        with open("dfgear_drop.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        return data
    except Exception as e:
        st.warning(f"DFGEAR ì ‘ì† ì‹¤íŒ¨: {e}")
        return []

# ì±„ë„ ì´ë¦„ì— ë”°ë¼ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
def categorize_channel(name):
    if "ë²¨ë§ˆì´ì–´" in name:
        return "ë²¨ë§ˆì´ì–´ êµ¬ì—­"
    elif "ì§€ë²¤" in name:
        return "ì§€ë²¤ êµ¬ì—­"
    elif "ë°±í•´" in name:
        return "ë°±í•´ êµ¬ì—­"
    elif "ë§ˆê³„" in name:
        return "ë§ˆê³„ êµ¬ì—­"
    elif "ì¤‘ì²œ" in name:
        return "ì¤‘ì²œ êµ¬ì—­"
    elif "ë°”í•˜ì´íŠ¸" in name:
        return "ë°”í•˜ì´íŠ¸ êµ¬ì—­"
    else:
        return "ê¸°íƒ€"

# ì •ì  JSON íŒŒì¼ë¡œë¶€í„° ë°ì´í„° ë¡œë“œ
def load_cached_data():
    try:
        url = "https://yourusername.github.io/dfgear-data/dfgear_drop.json"  # ì‹¤ì œ GitHub Pages ì£¼ì†Œë¡œ ë³€ê²½í•  ê²ƒ
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.warning(f"ì •ì  ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

# ë“í…œ ìƒìœ„ ì±„ë„ ì •ë¦¬
def get_hot_channels(data):
    df = pd.DataFrame(data)
    if df.empty:
        return pd.DataFrame()

    channel_counts = df["ì±„ë„"].value_counts()
    gear_counts = df.groupby("ì±„ë„")["ì¥ë¹„"].apply(list)
    categories = df.drop_duplicates("ì±„ë„").set_index("ì±„ë„")["ì¹´í…Œê³ ë¦¬"]

    result = pd.DataFrame({
        "ë“í…œ ìˆ˜": channel_counts,
        "ë“œëëœ ì¥ë¹„ ëª©ë¡": gear_counts,
        "ì¹´í…Œê³ ë¦¬": categories
    }).reset_index().rename(columns={"index": "ì±„ë„"})

    result.sort_values(by="ë“í…œ ìˆ˜", ascending=False, inplace=True)
    result.reset_index(drop=True, inplace=True)
    return result

# Streamlit ì›¹ ì•± ì‹œì‘
st.set_page_config(page_title="í—¬ì±„ë„ ìë™ ì¶”ì²œ", layout="wide")
st.title("ğŸ”¥ ë˜íŒŒ íƒœì´ˆ í—¬ì±„ë„ ìë™ ì¶”ì²œ ì‹œìŠ¤í…œ")

st.markdown("""
    **ì‹¤ì‹œê°„ ë“í…œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ í•«í•œ í—¬ ì±„ë„ì„ ì¶”ì²œí•©ë‹ˆë‹¤!!**

    > DFGEAR, ë””ì‹œíŒŒ ì¸ì¦ê¸€, ë˜íŒŒ ê³µì‹ ì»¤ë®¤ë‹ˆí‹° ë“±ì—ì„œ íƒœì´ˆ ë“í…œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ ìë™ ì¶”ì²œí•©ë‹ˆë‹¤!!
    > ë³¸ ì‹œìŠ¤í…œì€ GitHub Pages ë˜ëŠ” ì‹¤ì‹œê°„ í¬ë¡¤ë§ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤!!
""")

# ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹œë„
if st.sidebar.button("DFGEARì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° í¬ë¡¤ë§"):
    data = crawl_dfgear()
    if data:
        st.sidebar.success("ì‹¤ì‹œê°„ ë°ì´í„° í¬ë¡¤ë§ ì™„ë£Œ!!")
else:
    # ì •ì  JSON ë°ì´í„° ë¡œë“œ
    data = load_cached_data()

# ê²°ê³¼ ì¶œë ¥
if data:
    result = get_hot_channels(data)
    st.dataframe(result, use_container_width=True)

    if not result.empty:
        top_channel = result.iloc[0]
        st.success(f"ğŸ¯ ì§€ê¸ˆ ê°€ì¥ í•«í•œ ì±„ë„ì€ **{top_channel['ì±„ë„']}** ì…ë‹ˆë‹¤! ë“œë ìˆ˜: {top_channel['ë“í…œ ìˆ˜']}ê°œ")
else:
    st.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìºì‹œ íŒŒì¼ ê²½ë¡œë‚˜ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”!!")
