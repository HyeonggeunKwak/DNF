import pandas as pd
import streamlit as st
import requests
import json
import os
from bs4 import BeautifulSoup
from datetime import datetime
import subprocess

# ë˜íŒŒ ì•„ì¹´ë¼ì´ë¸Œ ì‹¤ì‹œê°„ ë“í…œ ì •ë³´ í¬ë¡¤ë§ í•¨ìˆ˜ ë° JSON ì €ì¥
def crawl_dnf_archive_and_save():
    url = "https://www.dnfarchive.com/board/view/notice"  # ë˜íŒŒ ì•„ì¹´ë¼ì´ë¸Œ ê²Œì‹œíŒ URL (ì˜ˆì‹œ)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        posts = soup.select(".board-list tbody tr")  # ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ ë¶€ë¶„

        data = []
        for post in posts:
            title = post.select_one(".title a").text.strip()
            link = post.select_one(".title a")["href"]
            
            # ì œëª©ì—ì„œ ì±„ë„ê³¼ ì¥ë¹„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
            channel = extract_channel_from_title(title)
            gear = extract_gear_from_title(title)
            
            data.append({"ì±„ë„": channel, "ì¥ë¹„": gear, "ì¶œì²˜": "ë˜íŒŒì•„ì¹´ë¼ì´ë¸Œ", "ë§í¬": link})

        # JSON íŒŒì¼ë¡œ ì €ì¥
        json_path = "docs/dnf_archive_drop.json"
        os.makedirs("docs", exist_ok=True)
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"[ìë™ ì €ì¥ ì™„ë£Œ] {json_path} ({len(data)}ê±´) â†’ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # ìë™ git ì»¤ë°‹ ë° í‘¸ì‹œ
        auto_git_push()

        return data
    except Exception as e:
        st.warning(f"ë˜íŒŒ ì•„ì¹´ë¼ì´ë¸Œ ì ‘ì† ì‹¤íŒ¨: {e}")
        return []

# ì±„ë„ ì •ë³´ ì¶”ì¶œ (ì˜ˆì‹œ: ì œëª©ì—ì„œ 'ì±„ë„ 1' ë“±ì˜ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ)
def extract_channel_from_title(title):
    if "ì±„ë„" in title:
        # ì˜ˆì‹œ: 'ì±„ë„ 1', 'ì±„ë„ 2' ë“±
        return title.split("ì±„ë„")[1].strip()
    return "ê¸°íƒ€"

# ì¥ë¹„ ì •ë³´ ì¶”ì¶œ (ì˜ˆì‹œ: ì œëª©ì—ì„œ 'íƒœì´ˆ'ë¼ëŠ” í‚¤ì›Œë“œë¡œ ì¥ë¹„ ì¶”ì¶œ)
def extract_gear_from_title(title):
    if "íƒœì´ˆ" in title:
        return "íƒœì´ˆ ì¥ë¹„"
    return "ê¸°íƒ€"

# ìë™ git ì»¤ë°‹ ë° í‘¸ì‹œ í•¨ìˆ˜
def auto_git_push():
    try:
        subprocess.run(["git", "add", "docs/dnf_archive_drop.json"], check=True)
        subprocess.run(["git", "commit", "-m", "ìë™ ì—…ë°ì´íŠ¸: dnf_archive_drop.json"], check=True)
        subprocess.run(["git", "push"], check=True)
        print("[Git Push ì™„ë£Œ] dnf_archive_drop.json ì—…ë¡œë“œ ì„±ê³µ!!")
    except subprocess.CalledProcessError as e:
        print(f"[Git ì˜¤ë¥˜] ìë™ ì»¤ë°‹ ë˜ëŠ” í‘¸ì‹œ ì‹¤íŒ¨: {e}")

# ì •ì  JSON íŒŒì¼ë¡œë¶€í„° ë°ì´í„° ë¡œë“œ
def load_cached_data():
    try:
        url = "https://hyeonggeunkwak.github.io/DNF/dnf_archive_drop.json"
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.warning(f"ì •ì  ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

# ë“í…œ ìƒìœ„ ì±„ë„ ì •ë¦¬
def get_hot_channels(data):
    df = pd.DataFrame(data)
    if df.empty:
        return pd.DataFrame()

    channel_counts = df["ì±„ë„"].value_counts()
    gear_counts = df.groupby("ì±„ë„")["ì¥ë¹„"].apply(list)
    result = pd.DataFrame({
        "ë“í…œ ìˆ˜": channel_counts,
        "ë“œëëœ ì¥ë¹„ ëª©ë¡": gear_counts,
    }).reset_index().rename(columns={"index": "ì±„ë„"})

    result.sort_values(by="ë“í…œ ìˆ˜", ascending=False, inplace=True)
    result.reset_index(drop=True, inplace=True)
    return result

# Streamlit ì›¹ ì•± ì‹œì‘
st.set_page_config(page_title="í—¬ì±„ë„ ìë™ ì¶”ì²œ", layout="wide")
st.title("ğŸ”¥ ë˜íŒŒ íƒœì´ˆ í—¬ì±„ë„ ìë™ ì¶”ì²œ ì‹œìŠ¤í…œ")

st.markdown(""" 
    **ì‹¤ì‹œê°„ ë“í…œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ í•«í•œ í—¬ ì±„ë„ì„ ì¶”ì²œí•©ë‹ˆë‹¤!!**
    
    > ë˜íŒŒ ì•„ì¹´ë¼ì´ë¸Œ ë° ë””ì‹œíŒŒ ë“±ì—ì„œ íƒœì´ˆ ë“í…œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ ìë™ ì¶”ì²œí•©ë‹ˆë‹¤!!
""")

# ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹œë„
if st.sidebar.button("ë˜íŒŒ ì•„ì¹´ë¼ì´ë¸Œì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° í¬ë¡¤ë§"):
    data = crawl_dnf_archive_and_save()
    if data:
        st.sidebar.success("ì‹¤ì‹œê°„ ë°ì´í„° í¬ë¡¤ë§ ë° ì €ì¥ ì™„ë£Œ!!")
else:
    # ì •ì  JSON ë°ì´í„° ë¡œë“œ
    data = load_cached_data()

# ê²°ê³¼ ì¶œë ¥
if data:
    result = get_hot_channels(data)
    st.dataframe(result, use_container_width=True)

    if not result.empty:
        top_channel = result.iloc[0]
        st.success(f"ğŸ¯ ì§€ê¸ˆ ê°€ì¥ í•«í•œ ì±„ë„ì€ **{top_channel['ì±„ë„']}** ì…ë‹ˆë‹¤! ë“œë ìˆ˜: {top_channel['ë“í…œ ìˆ˜']}ê°œ")
else:
    st.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìºì‹œ íŒŒì¼ ê²½ë¡œë‚˜ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”!!")